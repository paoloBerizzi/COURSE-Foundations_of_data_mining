{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Sequential()\n",
    "m.add(Dense(10,input_dim=19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define MLP class\n",
    "#binary classification problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    input_dim = None\n",
    "    \n",
    "    def __init__(self,neurons,activation,input_dim=None):\n",
    "    \n",
    "        self.neurons = neurons\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation_fun = relu\n",
    "            self.derivative_act = derivative_relu\n",
    "            \n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation_fun = sigmoid\n",
    "            self.derivative_act = derivative_sigmoid\n",
    "            \n",
    "        if input_dim is not None:\n",
    "            self.input_dim = input_dim\n",
    "            self.weights = np.asarray([ np.random.rand(inputd) for x in range(neurons)])\n",
    "            self.biases = np.asarray([np.random.randn(1) for x in range(neurons)])\n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Hidden Layer with '+str(self.neurons)+' neuros'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.01263329,  0.16859712, -0.96180611, -0.46940853,  1.22996626,\n",
       "        1.52439711,  0.79855695,  3.01874945, -0.82667687,  3.00935595])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prova\n",
    "inputd = 2\n",
    "x = np.array([1,2])\n",
    "weights =np.asarray([ np.random.rand(inputd) for x in range(10)])\n",
    "biases = np.asarray([np.random.randn() for x in range(10)])\n",
    "weights @ x + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \n",
    "    layers = []  #store all layers\n",
    "    z_array = [] #store all weighted sums  per layer\n",
    "    a_array = [] #store all activation units per layer a = act_fun(z)\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def add(self,Layer):\n",
    "        if Layer.input_dim is None: #infer input dimension \n",
    "            Layer.input_dim = self.layers[-1].neurons\n",
    "            Layer.weights = np.asarray([ np.random.rand(Layer.input_dim) for x in range(Layer.neurons)])\n",
    "            Layer.biases = np.asarray([ np.random.rand(1) for x in range(Layer.neurons)])\n",
    "        self.layers.append(Layer)\n",
    "      \n",
    "    #I use sgd to train the model \n",
    "    def train(self,inizialization,l_rate=0.1,batch_size=32):\n",
    "        #for epoch in #epochs\n",
    "            #batches <-- divide in batches\n",
    "            #for batch in batches\n",
    "                #forward propagate\n",
    "                #backpropagation of error\n",
    "                #update weights     \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    #find final output of the network and save all activations units and zetas\n",
    "    def feedforward(self,a): \n",
    "        #must inizialize these arrays?!\n",
    "        self.z_array.clear()\n",
    "        self.a_array.clear()\n",
    "        \n",
    "        a = a.T\n",
    "        for i in range(len(self.layers)):\n",
    "            z = np.dot(self.layers[i].weights,a) + self.layers[i].biases\n",
    "            self.z_array.append(z) \n",
    "            a = self.layers[i].activation_fun(z)\n",
    "            self.a_array.append(a)\n",
    "        return a\n",
    "    \n",
    "    #once you computed your output you have to find how good the result was\n",
    "    def backpropagation_hoc(self,x,y):\n",
    "        dr_cross =  derivative_bCrossEntroypy(self.a_array[-1],y)\n",
    "        dr_actfun = self.layers[-1].derivative_act(self.z_array[-1])\n",
    "        \n",
    "        dC_do = dr_cross*dr_actfun #o:output\n",
    "        \n",
    "        dC_dz = self.layers[-1].weights\n",
    "        pass\n",
    "      \n",
    "    \n",
    "    def backpropagation(self,x,y):\n",
    "        grad_b = [np.zeros(layer.biases.shape) for layer in self.layers] #dC/db\n",
    "        grad_w = [np.zeros(layer.weights.shape) for layer in self.layers] #dC/dw\n",
    "        \n",
    "        \n",
    "        #how the cost function is behaving ?\n",
    "        dr_cross =  derivative_bCrossEntroypy(self.a_array[-1],y)\n",
    "        dr_actfun = self.layers[-1].derivative_act(self.z_array[-1])\n",
    "        dC_do = dr_cross*dr_actfun\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(self.a_array[-2].T)\n",
    "        grad_b[-1] = dC_do\n",
    "        grad_w[-1] = dC_do.dot(self.a_array[-2].T) #errors second-last layer(10 outputs)\n",
    "    \n",
    "        for l in range(len(self.layers)-1,0,-1): #start from the last output layer and propagate the error\n",
    "            print(l,'ok')\n",
    "            #chain rule -> d^l = w^(l+1).T @ d^l+1 * derivate_actfun(z^l)\n",
    "            \n",
    "            dC_do = self.layers[l].weights.T.dot(dC_do)*self.layers[l].derivative_act(self.z_array[l-1])\n",
    "            #chain rule\n",
    "        \n",
    "            grad_b[l-1] = dC_do\n",
    "            grad_w[l-1] = np.dot(dC_do,self.a_array[l-1].transpose())\n",
    "        \n",
    "        return (grad_b,grad_w)\n",
    "            \n",
    "    \n",
    "    def __str__(self):\n",
    "        print(self.layers)\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.layers.clear()\n",
    "        self.a_array.clear()\n",
    "        self.z_array.clear()\n",
    "        \n",
    "    def print_layers(self):\n",
    "        for layer in self.layers:\n",
    "            print(layer)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6.94990052e-07],\n",
       "       [-1.45627998e-06],\n",
       "       [-1.03540290e-06],\n",
       "       [-1.10618257e-06],\n",
       "       [-1.09533409e-06],\n",
       "       [-6.44651739e-07],\n",
       "       [-4.84237588e-07],\n",
       "       [-1.24843701e-06],\n",
       "       [-3.66109367e-08],\n",
       "       [-8.07194606e-08]])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr_cross =  derivative_bCrossEntroypy(mlp.a_array[-1],1)\n",
    "dr_actfun = mlp.layers[-1].derivative_act(mlp.z_array[-1])\n",
    "\n",
    "dC_do = dr_cross*dr_actfun #o:output\n",
    "\n",
    "dC_dz2 = mlp.layers[-1].weights.T.dot(dC_do)\n",
    "dC_da1 = n\n",
    "dC_dw2  = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,1) and (10,10) not aligned: 1 (dim 1) != 10 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-247-2a72bb51b15e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#start from the second-last output layer and propagate the error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m        \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m        \u001b[0mdC_do\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdC_do\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mderivative_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m        \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdC_do\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            \u001b[0;31m#z = self.z_array[l]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,1) and (10,10) not aligned: 1 (dim 1) != 10 (dim 0)"
     ]
    }
   ],
   "source": [
    " for l in range(len(mlp.layers)-1,0,-1): #start from the second-last output layer and propagate the error\n",
    "        print(l)\n",
    "        dC_do = mlp.layers[l].weights.T.dot(dC_do)* mlp.layers[l].derivative_act(mlp.z_array[l-1])\n",
    "        np.dot(dC_do, mlp.a_array[l-1].transpose())\n",
    "            #z = self.z_array[l]\n",
    "            #derivative_actFun = self.layers[l].derivative_act(z)\n",
    "            \n",
    "            #chain rule\n",
    "           # dC_do =  np.dot(self.layers[l].weights.T,dC_do)*derivative_actFun\n",
    "            \n",
    "            #grad_b[l-1] = dC_do\n",
    "           # grad_w[l-1] = np.dot(dC_do,self.a_array[l-1].transpose())\n",
    "        \n",
    "       #return (grad_b,grad_w)\n",
    "            \n",
    "mlp.layers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp.add(Layer(10,input_dim=2,activation='relu'))\n",
    "mlp.add(Layer(10,activation='relu'))\n",
    "mlp.add(Layer(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(1,2)\n",
    "b = mlp.feedforward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999832]])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 ok\n",
      "1 ok\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (10,10) and (1,10) not aligned: 10 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-349-3e68a186d88c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-333-4aad3f30dc24>\u001b[0m in \u001b[0;36mbackpropagation\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mgrad_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdC_do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mgrad_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdC_do\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgrad_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (10,10) and (1,10) not aligned: 10 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "mlp.backpropagation(b,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.00833985e-06],\n",
       "       [-6.30366591e-06],\n",
       "       [-4.48185380e-06],\n",
       "       [-4.78823127e-06],\n",
       "       [-4.74127246e-06],\n",
       "       [-2.79044500e-06],\n",
       "       [-2.09607494e-06],\n",
       "       [-5.40399508e-06],\n",
       "       [-1.58474412e-07],\n",
       "       [-3.49402944e-07]])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.a_array\n",
    "dLout = derivative_bCrossEntroypy(b,1)*derivative_sigmoid(mlp.z_array[-1])\n",
    "dwout = mlp.a_array[-2].dot(dLout)\n",
    "\n",
    "\n",
    "dL2 = mlp.layers[-1].weights.T.dot(dLout)*derivative_relu(mlp.z_array[-2])\n",
    "dC_Dw = dL2.dot(mlp.a_array[-3])\n",
    "\n",
    "dC_Dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation and Loss functions.\n",
    "#####  I use Sigmoid as activation function for the hidden layers. For the output layer I use a sigmoid activation function. As  loss function I use binary cross entropy. The choice to use sigmoid as output activation function is due to the fact that  we are dealing with a binary classification problem and the sigmoid function will give us a value in the range (0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(x)/(1+np.exp(x))\n",
    "\n",
    "def derivative_sigmoid(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def derivative_relu(x):\n",
    "    x = x.flatten()\n",
    "    der = np.zeros(x.shape[0])\n",
    "    der[x>0] = 1\n",
    "    return der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_crossEntropy(o, y):#o: output ,y:target \n",
    "    return -(y*np.log(o) + (1-y)*np.log(1-o))\n",
    "def derivative_bCrossEntroypy(o,y):\n",
    "    return -(y/o+ (1-y)/(1-o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivative_bCrossEntroypy(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
